@inproceedings{bhc, author = {Heller, Katherine A. and Ghahramani, Zoubin}, title = {Bayesian Hierarchical Clustering}, year = {2005}, isbn = {1595931805}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1102351.1102389}, doi = {10.1145/1102351.1102389}, abstract = {We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast bottom-up approximate inference method for a Dirichlet process (i.e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperpa-rameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm.}, booktitle = {Proceedings of the 22nd International Conference on Machine Learning}, pages = {297â€“304}, numpages = {8}, location = {Bonn, Germany}, series = {ICML '05} }


@article{kmeans,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346830},
 author = {J. A. Hartigan and M. A. Wong},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {100--108},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Algorithm AS 136: A K-Means Clustering Algorithm},
 volume = {28},
 year = {1979}
}

@inproceedings{spec,
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 volume = {14},
 year = {2002}
}



@ARTICLE{spec0,  author={Jianbo Shi and Malik, J.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Normalized cuts and image segmentation},   year={2000},  volume={22},  number={8},  pages={888-905},  doi={10.1109/34.868688}}


@article{hier,
	abstract = {Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally ``connected,''while the other forms clusters that are optimally ``compact.''},
	author = {Johnson, Stephen C. },
	date = {1967/09/01},
	date-added = {2022-01-12 12:20:12 +0800},
	date-modified = {2022-01-12 12:20:12 +0800},
	doi = {10.1007/BF02289588},
	id = {Johnson1967},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {241--254},
	title = {Hierarchical clustering schemes},
	url = {https://doi.org/10.1007/BF02289588},
	volume = {32},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289588}}



@article{hier2,
abstract = {Post-genomic molecular biology has resulted in an explosion of data, providing measurements for large numbers of genes, proteins and metabolites. Time series experiments have become increasingly common, necessitating the development of novel analysis tools that capture the resulting data structure. Outlier measurements at one or more time points present a significant challenge, while potentially valuable replicate information is often ignored by existing techniques.},
author = {Cooke, Emma J. and Savage, Richard S. and Kirk, Paul DW and Darkins, Robert and Wild, David L.},
date = {2011/10/13},
date-added = {2022-01-13 08:40:07 +0800},
date-modified = {2022-01-13 08:40:07 +0800},
doi = {10.1186/1471-2105-12-399},
id = {Cooke2011},
isbn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {399},
title = {Bayesian hierarchical clustering for microarray time series data with replicates and outlier measurements},
url = {https://doi.org/10.1186/1471-2105-12-399},
volume = {12},
year = {2011},
bdsk-url-1 = {https://doi.org/10.1186/1471-2105-12-399}}

@book{duda2012pattern,
  title={Pattern Classification},
  author={Duda, R.O. and Hart, P.E. and Stork, D.G.},
  isbn={9781118586006},
  url={https://books.google.com.hk/books?id=Br33IRC3PkQC},
  year={2012},
  publisher={Wiley}
}
